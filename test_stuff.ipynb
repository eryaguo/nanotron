{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dmitrii/Documents/uni/masters/3/dsl/data-processing/nanotron/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "from typing import Dict, cast\n",
    "\n",
    "import datasets\n",
    "import numpy as np\n",
    "from nanotron import logging\n",
    "from nanotron.config import (\n",
    "    DataArgs,\n",
    "    DatasetStageArgs,\n",
    "    PretrainDatasetsArgs,\n",
    ")\n",
    "from nanotron.dataloader import (\n",
    "    DataCollatorForCLM,\n",
    "    clm_process,\n",
    "    get_dataloader_worker_init,\n",
    "    get_datasets,\n",
    "    get_train_dataloader,\n",
    "    vqa_process,\n",
    ")\n",
    "from nanotron.helpers import (\n",
    "    compute_remain_train_steps_of_a_data_stage_from_ckp,\n",
    "    get_consumed_train_samples_of_a_data_stage_from_ckp,\n",
    ")\n",
    "from nanotron.logging import log_rank\n",
    "from nanotron.parallel.pipeline_parallel.utils import get_input_output_pp_ranks\n",
    "from nanotron.trainer import DistributedTrainer\n",
    "from nanotron.utils import main_rank_first\n",
    "# from torch.utils.data import DataLoader\n",
    "\n",
    "from transformers import AutoProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"cmarkea/doc-vqa\"\n",
    "dataset_config = None\n",
    "dataset_splits = \"train\"\n",
    "tokenizer_path = \"HuggingFaceM4/Idefics3-8B-Llama3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/9688 [00:00<?, ? examples/s]"
     ]
    }
   ],
   "source": [
    "raw_dataset = get_datasets(\n",
    "    hf_dataset_or_datasets=dataset,\n",
    "    hf_dataset_config_name=dataset_config,\n",
    "    splits=dataset_splits,\n",
    ")[\"train\"]\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(tokenizer_path, size= {\"longest_edge\": 2*364})\n",
    "train_dataset = vqa_process(\n",
    "    raw_dataset=raw_dataset,\n",
    "    processor=processor,\n",
    "    dataset_processing_num_proc_per_process=1,\n",
    "    dataset_overwrite_cache=True,\n",
    "    sequence_length=1024,\n",
    ")\n",
    "\n",
    "\n",
    "# We load the processed dataset on the ranks requiring it\n",
    "# dataloader = get_train_dataloader(\n",
    "#     train_dataset=train_dataset,\n",
    "#     sequence_length=trainer.sequence_length,\n",
    "#     parallel_context=trainer.parallel_context,\n",
    "#     input_pp_rank=input_pp_rank,\n",
    "#     output_pp_rank=output_pp_rank,\n",
    "#     micro_batch_size=trainer.micro_batch_size,\n",
    "#     consumed_train_samples=consumed_train_samples,\n",
    "#     dataloader_num_workers=data.num_loading_workers,\n",
    "#     seed_worker=data.seed,\n",
    "#     dataloader_drop_last=True,\n",
    "#     dataset_columns=[\"input_ids\", \"pixel_values\"]\n",
    "# )\n",
    "\n",
    "# Check if we have enough samples for train_steps\n",
    "# total_tokens_dataset = len(dataloader.dataset) * trainer.sequence_length\n",
    "# num_tokens_needed_for_training = (\n",
    "#     num_remaining_train_steps * trainer.global_batch_size * trainer.sequence_length\n",
    "# )\n",
    "# assert num_tokens_needed_for_training <= total_tokens_dataset, (\n",
    "#     f\"Dataset is too small for steps ({total_tokens_dataset} < {num_tokens_needed_for_training}), \"\n",
    "#     f\"Try train_steps<={len(dataloader.dataset) // trainer.global_batch_size + trainer.iteration_step}\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pillow\n",
      "  Downloading pillow-11.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (4.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pillow\n",
      "Successfully installed pillow-11.0.0\n",
      "\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nanotron.dataloader import (\n",
    "    clm_process,\n",
    "    dummy_infinite_data_generator,\n",
    "    get_datasets,\n",
    "    get_train_dataloader,\n",
    ")\n",
    "\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"cmarkea/doc-vqa\"\n",
    "tokenizer_path = \"robot-test/dummy-tokenizer-wordlevel\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# os.environ[\"HF_ENDPOINT\"] = \"http://localhost:5564\"\n",
    "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c01bb81dfd134584ae1b614e1a0f76d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/3.87k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee1524a26c6c439c81c2b60f6100d62a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00013.parquet:   0%|          | 0.00/142M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a030a4575daa425593da9a62a24d7193",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00001-of-00013.parquet:   0%|          | 0.00/134M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40d4e3c362b74f9d91f417636da46aff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00002-of-00013.parquet:   0%|          | 0.00/144M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68d4eff3246948afa5ca13cd8f331377",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00003-of-00013.parquet:   0%|          | 0.00/139M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4c7cd78bf4145eb85e6af1579bd34d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00004-of-00013.parquet:   0%|          | 0.00/141M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9e81446a2414b6396c3d37d9cae074b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00005-of-00013.parquet:   0%|          | 0.00/138M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b5bb4629fd340ccb0bdecb1cc3a67e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00006-of-00013.parquet:   0%|          | 0.00/142M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bb3ab4ba6194181a1ae23914c5d11d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00007-of-00013.parquet:   0%|          | 0.00/147M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbea2581f3134340b3c15088b3aa6065",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00008-of-00013.parquet:   0%|          | 0.00/140M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16ce8d6ee96b45c0a3d12a30dde76c98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00009-of-00013.parquet:   0%|          | 0.00/147M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dabc00a6c62b442898626fd1b000877b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00010-of-00013.parquet:   0%|          | 0.00/132M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9390cd74091e434098fd740a0ee14ad1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00011-of-00013.parquet:   0%|          | 0.00/103M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ff5edb730a544ad9e01844611fbeb04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00012-of-00013.parquet:   0%|          | 0.00/105M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be0d476330d542648264473cc905af14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00004.parquet:   0%|          | 0.00/119M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7200a853139a4411beb27d7a0adcd5b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00001-of-00004.parquet:   0%|          | 0.00/111M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa83c61851c54c05917e3a360e5478b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00002-of-00004.parquet:   0%|          | 0.00/106M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1f6d0a969ad421787b522624c17f578",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00003-of-00004.parquet:   0%|          | 0.00/95.6M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a98329bbc1148159ac64ccd7fe2d5bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/9688 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "445b60bc02b94b3da4079e325058fea0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/2421 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "raw_dataset = get_datasets(\n",
    "    hf_dataset_or_datasets=dataset,\n",
    "    hf_dataset_config_name=None,\n",
    "    splits=\"train\",\n",
    ")[\"train\"]\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, AutoModelForVision2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = AutoProcessor.from_pretrained(\"HuggingFaceM4/Idefics3-8B-Llama3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex = raw_dataset[0]\n",
    "messages = []\n",
    "for i, x in enumerate(ex[\"qa\"][\"en\"]):\n",
    "    user_message = {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": x[\"question\"]},\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    if i == 0:\n",
    "        user_message[\"content\"].append(\n",
    "            {\"type\": \"image\"},\n",
    "        )\n",
    "\n",
    "    messages.append(user_message)\n",
    "    assistant_message = {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": x[\"answer\"]},\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    messages.append(assistant_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = processor.apply_chat_template(messages, add_generation_prompt=True)\n",
    "inputs = processor(text=prompt, images=[ex[\"image\"]], return_tensors=\"pt\")\n",
    "inputs = {k: v.to(\"cpu\") for k, v in inputs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = processor(text=prompt, images=[ex[\"image\"]], return_tensors=\"pt\", max_length=128, padding=\"longest\", truncation=True)\n",
    "inputs[\"input_ids\"].shape\n",
    "inputs[\"attention_mask\"].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "UndefinedError",
     "evalue": "'dict object' has no attribute 'role'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUndefinedError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 14\u001b[0m\n\u001b[1;32m      1\u001b[0m pretraining_data \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      2\u001b[0m     {\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# ... more image-text pairs\u001b[39;00m\n\u001b[1;32m     11\u001b[0m ]\n\u001b[0;32m---> 14\u001b[0m text \u001b[38;5;241m=\u001b[39m processor\u001b[38;5;241m.\u001b[39mapply_chat_template(pretraining_data, add_generation_prompt\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/transformers/processing_utils.py:1101\u001b[0m, in \u001b[0;36mProcessorMixin.apply_chat_template\u001b[0;34m(self, conversation, chat_template, tokenize, **kwargs)\u001b[0m\n\u001b[1;32m   1095\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1096\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1097\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo chat template is set for this processor. Please either set the `chat_template` attribute, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1098\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor provide a chat template as an argument. See \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1099\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://huggingface.co/docs/transformers/main/en/chat_templating for more information.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1100\u001b[0m         )\n\u001b[0;32m-> 1101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mapply_chat_template(\n\u001b[1;32m   1102\u001b[0m     conversation, chat_template\u001b[38;5;241m=\u001b[39mchat_template, tokenize\u001b[38;5;241m=\u001b[39mtokenize, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m   1103\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1869\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.apply_chat_template\u001b[0;34m(self, conversation, tools, documents, chat_template, add_generation_prompt, continue_final_message, tokenize, padding, truncation, max_length, return_tensors, return_dict, return_assistant_tokens_mask, tokenizer_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m   1867\u001b[0m     all_generation_indices\u001b[38;5;241m.\u001b[39mappend(generation_indices)\n\u001b[1;32m   1868\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1869\u001b[0m     rendered_chat \u001b[38;5;241m=\u001b[39m compiled_template\u001b[38;5;241m.\u001b[39mrender(\n\u001b[1;32m   1870\u001b[0m         messages\u001b[38;5;241m=\u001b[39mchat,\n\u001b[1;32m   1871\u001b[0m         tools\u001b[38;5;241m=\u001b[39mtool_schemas,\n\u001b[1;32m   1872\u001b[0m         documents\u001b[38;5;241m=\u001b[39mdocuments,\n\u001b[1;32m   1873\u001b[0m         add_generation_prompt\u001b[38;5;241m=\u001b[39madd_generation_prompt,\n\u001b[1;32m   1874\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtemplate_kwargs,\n\u001b[1;32m   1875\u001b[0m     )\n\u001b[1;32m   1876\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m continue_final_message:\n\u001b[1;32m   1877\u001b[0m     final_message \u001b[38;5;241m=\u001b[39m chat[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/jinja2/environment.py:1304\u001b[0m, in \u001b[0;36mTemplate.render\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1302\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvironment\u001b[38;5;241m.\u001b[39mconcat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot_render_func(ctx))  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m   1303\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m-> 1304\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvironment\u001b[38;5;241m.\u001b[39mhandle_exception()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/jinja2/environment.py:939\u001b[0m, in \u001b[0;36mEnvironment.handle_exception\u001b[0;34m(self, source)\u001b[0m\n\u001b[1;32m    934\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Exception handling helper.  This is used internally to either raise\u001b[39;00m\n\u001b[1;32m    935\u001b[0m \u001b[38;5;124;03mrewritten exceptions or return a rendered traceback for the template.\u001b[39;00m\n\u001b[1;32m    936\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    937\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdebug\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m rewrite_traceback_stack\n\u001b[0;32m--> 939\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m rewrite_traceback_stack(source\u001b[38;5;241m=\u001b[39msource)\n",
      "File \u001b[0;32m<template>:1\u001b[0m, in \u001b[0;36mtop-level template code\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/jinja2/sandbox.py:327\u001b[0m, in \u001b[0;36mSandboxedEnvironment.getattr\u001b[0;34m(self, obj, attribute)\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Subscribe an object from sandboxed code and prefer the\u001b[39;00m\n\u001b[1;32m    324\u001b[0m \u001b[38;5;124;03mattribute.  The attribute passed *must* be a bytestring.\u001b[39;00m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    326\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 327\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(obj, attribute)\n\u001b[1;32m    328\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[1;32m    329\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mUndefinedError\u001b[0m: 'dict object' has no attribute 'role'"
     ]
    }
   ],
   "source": [
    "pretraining_data = [\n",
    "    {\n",
    "        \"image\": None,\n",
    "        \"text\": \"A view of the Statue of Liberty in New York.\"\n",
    "    },\n",
    "    {\n",
    "        \"image\": None,\n",
    "        \"text\": \"The skyline of Chicago during sunset.\"\n",
    "    },\n",
    "    # ... more image-text pairs\n",
    "]\n",
    "\n",
    "\n",
    "text = processor.apply_chat_template(pretraining_data, add_generation_prompt=True)\n",
    "# inputs = processor(images=images, text=text, return_tensors=\"pt\").to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_c = [\"2313212\"] * len(raw_dataset)\n",
    "\n",
    "new_dataset = raw_dataset.add_column(\"new_c\", new_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c9236b3f6934f3d87fbfdbcf4505a97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 257 (num_proc=16):   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset = clm_process(\n",
    "    raw_dataset=new_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    text_column_name=\"new_c\",\n",
    "    dataset_processing_num_proc_per_process=16,\n",
    "    dataset_overwrite_cache=False,\n",
    "    sequence_length=256,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "320"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
